{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "206a33a4-5737-4a5b-b089-5cc1eb708a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894ad5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64feef00-fbad-400b-94dc-e96c72fce220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用GPU\n",
    "    print(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # 使用CPU\n",
    "    print(\"cpu\")\n",
    "#input_file_path\n",
    "#number of subset\n",
    "#test_time_step\n",
    "#epoch\n",
    "#patience\n",
    "#hyperpara\n",
    "#output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e17e5d-c944-471e-87d4-2dceb5a82dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data, file name:  ./牌局/collection_onehot_handfeature_cleaned_0.xlsx\n",
      "執行時間：460.651228 秒\n",
      "Fetching data, file name:  ./牌局/collection_onehot_handfeature_cleaned_1.xlsx\n",
      "執行時間：497.282576 秒\n"
     ]
    }
   ],
   "source": [
    "input_file_path = './牌局/collection_onehot_handfeature_cleaned_'\n",
    "start = time.time()\n",
    "print(\"Fetching data, file name: \", (input_file_path+str(0)+\".xlsx\"))\n",
    "raw_data = pd.read_excel(input_file_path+str(0)+\".xlsx\")\n",
    "print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "for i in range(1, 2):\n",
    "    start = time.time()\n",
    "    print(\"Fetching data, file name: \", (input_file_path+str(i)+\".xlsx\"))\n",
    "    raw_data = pd.concat([raw_data, pd.read_excel(input_file_path+str(i)+\".xlsx\")], axis = 0, ignore_index = True)\n",
    "    print(\"執行時間：%f 秒\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52582757-d58f-47f2-8f94-2846892c9fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_and_pad(mydata):\n",
    "    start = time.time()\n",
    "    print(\"spliting dataset\")\n",
    "    current_sample_id = ''\n",
    "    row_data_X = []\n",
    "    row_data_y = []\n",
    "    X = []\n",
    "    y = []\n",
    "    decoded_label = []\n",
    "    sequence_lengths = []\n",
    "    step = 0\n",
    "    max_step = 0\n",
    "    test_time_step = 1000\n",
    "    test_size = 0\n",
    "    cnt = 0\n",
    "    flag = True\n",
    "    for index, row in mydata.iterrows():\n",
    "        if row['index'] != '':\n",
    "            if (row['style'] != \"nat\"):\n",
    "                continue\n",
    "            if row['index'] == current_sample_id:\n",
    "                row_data_X.append(row.iloc[57:188].tolist())\n",
    "                row_data_y.append(row.iloc[189:227].tolist())\n",
    "                if flag is True:\n",
    "                    decoded_label.append(row.iloc[0:5].tolist())\n",
    "                    cnt = cnt+1\n",
    "                step = step+1\n",
    "            else:\n",
    "                if row_data_X != []:\n",
    "                    X.append(row_data_X)\n",
    "                    y.append(row_data_y)\n",
    "                    sequence_lengths.append(step)\n",
    "                    if step > max_step:\n",
    "                        max_step = step\n",
    "                step = 1\n",
    "                row_data_X = []\n",
    "                row_data_y = []\n",
    "                row_data_X.append(row.iloc[57:188].tolist())\n",
    "                row_data_y.append(row.iloc[189:227].tolist())\n",
    "                if cnt >= test_time_step:\n",
    "                    flag = False\n",
    "                if flag is True:\n",
    "                    decoded_label.append(row.iloc[0:5].tolist())\n",
    "                    cnt = cnt+1\n",
    "                    test_size = test_size+1\n",
    "                current_sample_id = row['index']\n",
    "    if row_data_X != []:\n",
    "        X.append(row_data_X)\n",
    "        y.append(row_data_y)\n",
    "        sequence_lengths.append(step)\n",
    "        if step > max_step:\n",
    "            max_step = step\n",
    "    print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "    start = time.time()\n",
    "    print(\"padding dataset\")\n",
    "    void_X = [-1] * 131\n",
    "    void_y = [-1] * 38\n",
    "    for i in range(0, len(sequence_lengths)):\n",
    "        padding_length = max_step - sequence_lengths[i]\n",
    "        X[i].extend([void_X] * padding_length)\n",
    "        y[i].extend([void_y] * padding_length)\n",
    "    print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "    return X, y, sequence_lengths, decoded_label, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86ecd40-2e66-4f9e-8776-16df7f2fc4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliting dataset\n",
      "執行時間：29.612218 秒\n",
      "padding dataset\n",
      "執行時間：0.043907 秒\n"
     ]
    }
   ],
   "source": [
    "X, y, sequence_lengths, decoded_label, test_size = split_and_pad(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d31c75-6c8c-45d2-9b4c-ce54959fccab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test spliting\n",
      "total train_size:  67872 ; total valid_size:  16968 ; total test_size:  348\n",
      "執行時間：0.260351 秒\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"train test spliting\")\n",
    "test_X = X[:test_size]\n",
    "test_y = y[:test_size]\n",
    "n_sample = len(X)\n",
    "train_size = int((len(X) - test_size)*0.8)\n",
    "valid_size = len(X) - test_size - train_size\n",
    "combined = list(zip(X[test_size:], y[test_size:], sequence_lengths[test_size:]))\n",
    "random.shuffle(combined)\n",
    "train_X, train_y, train_seq_len = zip(*combined[:train_size])\n",
    "valid_X, valid_y, valid_seq_len = zip(*combined[train_size:])\n",
    "print(\"total train_size: \", train_size, \"; total valid_size: \", valid_size, \"; total test_size: \", test_size)\n",
    "print(\"執行時間：%f 秒\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdeaae42-78e7-486f-b639-3bba045a41d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=131, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 38)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_X, train_y, train_seq_len,\n",
    "                valid_X, valid_y, valid_seq_len, criterion, optimizer, epochs = 100, patience = 6):\n",
    "    total_time = time.time()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = float('inf')\n",
    "    last_valid_loss = float('inf')\n",
    "    patience_cnt = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_cnt = 0\n",
    "        start = time.time()\n",
    "        for inputs, labels in zip(train_X, train_y):\n",
    "            class_id = []\n",
    "            max_bid = 0\n",
    "            for time_step in range(0, train_seq_len[batch_cnt]):\n",
    "                cur_bid = np.argmax(labels[time_step])\n",
    "                if ((cur_bid > 2) & (cur_bid < max_bid)):\n",
    "                    train_seq_len[batch_cnt] = time_step+1\n",
    "                    break\n",
    "                class_id.append(cur_bid)\n",
    "            labels = torch.tensor(class_id, dtype=torch.long).cuda()\n",
    "            inputs = torch.tensor(inputs[:train_seq_len[batch_cnt]]\n",
    "                                  , dtype=torch.float).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.cpu()\n",
    "            running_loss += abs(loss.item())\n",
    "            batch_cnt = batch_cnt+1\n",
    "            if batch_cnt % 10000 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_cnt}/{len(train_X)}')\n",
    "                print(f'Loss: {running_loss/batch_cnt}')\n",
    "                print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "                start = time.time()\n",
    "\n",
    "        start = time.time()\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        batch_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in zip(valid_X, valid_y):\n",
    "                inputs = torch.tensor(inputs[:valid_seq_len[batch_cnt]]\n",
    "                                      , dtype=torch.float).cuda()\n",
    "                class_id = []\n",
    "                for time_step in range(0, valid_seq_len[batch_cnt]):\n",
    "                    class_id.append(np.argmax(labels[time_step]))\n",
    "                labels = torch.tensor(class_id, dtype=torch.long).cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss.cpu()\n",
    "                valid_loss += abs(loss.item())\n",
    "                batch_cnt = batch_cnt+1\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model_state = model.state_dict()\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Training Loss: {running_loss/len(train_X)}')\n",
    "        print(f'Validation Loss: {valid_loss/len(valid_X)}')\n",
    "        print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "        train_losses.append(running_loss/len(train_X))\n",
    "        valid_losses.append(valid_loss/len(valid_X))\n",
    "        if (last_valid_loss < valid_loss):\n",
    "            patience_cnt = patience_cnt+1\n",
    "        else:\n",
    "            patience_cnt = 0\n",
    "            last_valid_loss = valid_loss\n",
    "        if (patience_cnt > patience):\n",
    "            break\n",
    "    return model, best_model_state, train_losses, valid_losses, time.time()-total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0da65631-921d-4aab-982d-4253c081b4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_decode(model, best_model_state, hidden_size, num_layers, dropout, file_prefix):\n",
    "    print(\"testing dataset\")\n",
    "    start = time.time()\n",
    "    test_loss = 0\n",
    "    test_acc_true = 0\n",
    "    test_acc_false = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_model = LSTMModel(hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    best_model.eval()\n",
    "    with gzip.GzipFile(file_prefix+'_best_model.pgz', 'w') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    with gzip.GzipFile(file_prefix+'_final_model.pgz', 'w') as f:\n",
    "        pickle.dump(model, f)\n",
    "    results = []\n",
    "    batch_cnt = 0\n",
    "    for inputs, labels in zip(test_X, test_y):\n",
    "        inputs = torch.tensor(inputs[:sequence_lengths[batch_cnt]], dtype=torch.float)\n",
    "        class_id = []\n",
    "        for time_step in range(0, sequence_lengths[batch_cnt]):\n",
    "            class_id.append(np.argmax(labels[time_step]))\n",
    "        labels = torch.tensor(class_id, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            predictions = best_model(inputs)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += abs(loss.item())\n",
    "            for _cnt in range(0, sequence_lengths[batch_cnt]):\n",
    "                results.append(predictions[_cnt].tolist())\n",
    "                if (np.argmax(predictions[_cnt]) == class_id[_cnt]):\n",
    "                    test_acc_true = test_acc_true+1\n",
    "                else:\n",
    "                    test_acc_false = test_acc_false+1\n",
    "        batch_cnt = batch_cnt+1\n",
    "    test_loss = test_loss/batch_cnt\n",
    "    test_acc = test_acc_true/(test_acc_true+test_acc_false)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    excel_file = file_prefix+'_full.xlsx'\n",
    "    results_df.to_excel(excel_file, index=False)\n",
    "    print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "    print(\"decoding dataset\")\n",
    "    start = time.time()\n",
    "    decoded_results = []\n",
    "    batch_cnt = 0\n",
    "    for index, row in results_df.iterrows():\n",
    "        row_data = []\n",
    "        row_data.append(decoded_label[batch_cnt])\n",
    "        max_val = row[0]\n",
    "        max_pos = 0\n",
    "        for i in range(1, len(row)):\n",
    "            if(row[i] > max_val):\n",
    "                max_val = row[i]\n",
    "                max_pos = i\n",
    "        row_data.append(max_pos)\n",
    "        decoded_results.append(row_data)\n",
    "        batch_cnt = batch_cnt+1\n",
    "    decoded_results_df = pd.DataFrame(decoded_results)\n",
    "    excel_file = file_prefix+'_decoded.xlsx'\n",
    "    decoded_results_df.to_excel(excel_file, index=False)\n",
    "    print(\"執行時間：%f 秒\" % (time.time() - start))\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1939f697-6442-49d3-8c26-07f496d0816d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with columns ['optimizer', 'dropout', 'lr', 'hidden_size', 'num_layers', 'test_loss', 'test_acc', 'total_time', '1.train_loss', '2.train_loss', '3.train_loss', '4.train_loss', '5.train_loss', '6.train_loss', '7.train_loss', '8.train_loss', '9.train_loss', '10.train_loss', '11.train_loss', '12.train_loss', '13.train_loss', '14.train_loss', '15.train_loss', '16.train_loss', '17.train_loss', '18.train_loss', '19.train_loss', '20.train_loss', '21.train_loss', '22.train_loss', '23.train_loss', '24.train_loss', '25.train_loss', '26.train_loss', '27.train_loss', '28.train_loss', '29.train_loss', '30.train_loss', '31.train_loss', '32.train_loss', '33.train_loss', '34.train_loss', '35.train_loss', '36.train_loss', '37.train_loss', '38.train_loss', '39.train_loss', '40.train_loss', '41.train_loss', '42.train_loss', '43.train_loss', '44.train_loss', '45.train_loss', '46.train_loss', '47.train_loss', '48.train_loss', '49.train_loss', '50.train_loss', '51.train_loss', '52.train_loss', '53.train_loss', '54.train_loss', '55.train_loss', '56.train_loss', '57.train_loss', '58.train_loss', '59.train_loss', '60.train_loss', '61.train_loss', '62.train_loss', '63.train_loss', '64.train_loss', '65.train_loss', '66.train_loss', '67.train_loss', '68.train_loss', '69.train_loss', '70.train_loss', '71.train_loss', '72.train_loss', '73.train_loss', '74.train_loss', '75.train_loss', '76.train_loss', '77.train_loss', '78.train_loss', '79.train_loss', '80.train_loss', '81.train_loss', '82.train_loss', '83.train_loss', '84.train_loss', '85.train_loss', '86.train_loss', '87.train_loss', '88.train_loss', '89.train_loss', '90.train_loss', '91.train_loss', '92.train_loss', '93.train_loss', '94.train_loss', '95.train_loss', '96.train_loss', '97.train_loss', '98.train_loss', '99.train_loss', '100.train_loss', '1.valid_loss', '2.valid_loss', '3.valid_loss', '4.valid_loss', '5.valid_loss', '6.valid_loss', '7.valid_loss', '8.valid_loss', '9.valid_loss', '10.valid_loss', '11.valid_loss', '12.valid_loss', '13.valid_loss', '14.valid_loss', '15.valid_loss', '16.valid_loss', '17.valid_loss', '18.valid_loss', '19.valid_loss', '20.valid_loss', '21.valid_loss', '22.valid_loss', '23.valid_loss', '24.valid_loss', '25.valid_loss', '26.valid_loss', '27.valid_loss', '28.valid_loss', '29.valid_loss', '30.valid_loss', '31.valid_loss', '32.valid_loss', '33.valid_loss', '34.valid_loss', '35.valid_loss', '36.valid_loss', '37.valid_loss', '38.valid_loss', '39.valid_loss', '40.valid_loss', '41.valid_loss', '42.valid_loss', '43.valid_loss', '44.valid_loss', '45.valid_loss', '46.valid_loss', '47.valid_loss', '48.valid_loss', '49.valid_loss', '50.valid_loss', '51.valid_loss', '52.valid_loss', '53.valid_loss', '54.valid_loss', '55.valid_loss', '56.valid_loss', '57.valid_loss', '58.valid_loss', '59.valid_loss', '60.valid_loss', '61.valid_loss', '62.valid_loss', '63.valid_loss', '64.valid_loss', '65.valid_loss', '66.valid_loss', '67.valid_loss', '68.valid_loss', '69.valid_loss', '70.valid_loss', '71.valid_loss', '72.valid_loss', '73.valid_loss', '74.valid_loss', '75.valid_loss', '76.valid_loss', '77.valid_loss', '78.valid_loss', '79.valid_loss', '80.valid_loss', '81.valid_loss', '82.valid_loss', '83.valid_loss', '84.valid_loss', '85.valid_loss', '86.valid_loss', '87.valid_loss', '88.valid_loss', '89.valid_loss', '90.valid_loss', '91.valid_loss', '92.valid_loss', '93.valid_loss', '94.valid_loss', '95.valid_loss', '96.valid_loss', '97.valid_loss', '98.valid_loss', '99.valid_loss', '100.valid_loss', 'epoch'],\n",
      "Empty CSV file ./093012_hyperpara.csv has been created.\n",
      "current model-- 0 th exp:  hidden_size =  512 num_layers =  8 dropout =  0.28732073011293013 optimizer =  adam lr =  0.01\n",
      "Epoch 1/5\n",
      "Training Loss: 2.0517511280019978\n",
      "Validation Loss: 1.6208916008472443\n",
      "執行時間：0.024361 秒\n",
      "Epoch 2/5\n",
      "Training Loss: 3.34383426535396\n",
      "Validation Loss: 1.7808823194354773\n",
      "執行時間：0.023957 秒\n",
      "Epoch 3/5\n",
      "Training Loss: 3.434104836889076\n",
      "Validation Loss: 1.853550398722291\n",
      "執行時間：0.025400 秒\n",
      "Epoch 4/5\n",
      "Training Loss: 3.4328034718265914\n",
      "Validation Loss: 1.674813974648714\n",
      "執行時間：0.021410 秒\n",
      "Epoch 5/5\n",
      "Training Loss: 3.4287416159947344\n",
      "Validation Loss: 1.832900371402502\n",
      "執行時間：0.024152 秒\n",
      "testing dataset\n",
      "執行時間：84.153219 秒\n",
      "decoding dataset\n",
      "執行時間：0.118099 秒\n"
     ]
    }
   ],
   "source": [
    "n_sample = 1\n",
    "batch_size = int(train_size*0.1)\n",
    "valid_size = int(valid_size*0.1)\n",
    "opt = ['adam', 'amsgrad']\n",
    "dropout = (0.1, 0.5)\n",
    "alpha = (-5, -1)\n",
    "hidden_size = (6, 12)\n",
    "num_layers = (2, 10)\n",
    "\n",
    "file_prefix = './093012'\n",
    "column_names = ['optimizer', 'dropout', 'lr', 'hidden_size', 'num_layers', 'test_loss', 'test_acc', 'total_time']\n",
    "additional_columns = [f\"{i}.train_loss\" for i in range(1, 101)] + [f\"{i}.valid_loss\" for i in range(1, 101)] + ['epoch']\n",
    "column_names.extend(additional_columns)\n",
    "print(f\"with columns {column_names},\")\n",
    "para_df = pd.DataFrame(columns=column_names)\n",
    "para_df.to_csv((file_prefix+\"_hyperpara.csv\"), index=False)\n",
    "print(f\"Empty CSV file {file_prefix}_hyperpara.csv has been created.\")\n",
    "for i in range(0, n_sample):\n",
    "    paras = [None]*5\n",
    "    paras[0] = opt[np.random.randint(0, 2)]\n",
    "    paras[1] = np.random.uniform(dropout[0], dropout[1])\n",
    "    paras[2] = 10 ** (np.random.randint(alpha[0], alpha[1]+1))\n",
    "    paras[3] = 2 ** (np.random.randint(hidden_size[0], hidden_size[1]))\n",
    "    paras[4] = np.random.randint(num_layers[0], num_layers[1])\n",
    "    # asign specific para here\n",
    "    model = LSTMModel(hidden_size = paras[3], num_layers = paras[4], dropout = paras[1]).cuda()\n",
    "    print(\"current model--\", i, \"th exp: \", \"hidden_size = \", paras[3], \"num_layers = \", paras[4], \"dropout = \", paras[1], \"optimizer = \", paras[0], \"lr = \", paras[2])\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    if paras[0] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=paras[2])\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=paras[2], amsgrad = True)\n",
    "    model, best_model_state, train_losses, valid_losses, total_time = train_model(model, train_X[:batch_size], train_y[:batch_size], train_seq_len[:batch_size],\n",
    "                                          valid_X[:valid_size], valid_y[:valid_size], valid_seq_len[:valid_size], criterion, optimizer, epochs = 5, patience = 3)\n",
    "    test_loss, test_acc = test_decode(model, best_model_state, paras[3], paras[4], paras[1], file_prefix+\"_\"+str(i))\n",
    "    row_data = {\n",
    "        'optimizer': paras[0],\n",
    "        'dropout': paras[1],\n",
    "        'lr': paras[2],\n",
    "        'hidden_size': paras[3],\n",
    "        'num_layers': paras[4],\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc' : test_acc,\n",
    "        'total_time' : total_time\n",
    "    }\n",
    "    _epoch = 0\n",
    "    for j in range(0, 100):\n",
    "        if j < len(train_losses):\n",
    "            row_data[f'{j+1}.train_loss'] = train_losses[j]\n",
    "            _epoch = j+1\n",
    "        else:\n",
    "            row_data[f'{j+1}.train_loss'] = \"nan\"\n",
    "    for k in range(0, 100):\n",
    "        if k < len(valid_losses):\n",
    "            row_data[f'{k+1}.valid_loss'] = valid_losses[k]\n",
    "        else:\n",
    "            row_data[f'{k+1}.valid_loss'] = \"nan\"\n",
    "    row_data['epoch'] = _epoch\n",
    "    para_df = pd.DataFrame([row_data])\n",
    "    para_df.to_csv(file_prefix+\"_hyperpara.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e721b-a035-4665-b0d5-32a47cb59171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
